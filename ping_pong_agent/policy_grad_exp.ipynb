{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.functional import one_hot, logsigmoid, log_softmax, softmax\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from policy_net import PongAgent\n",
    "import gym\n",
    "\n",
    "###### set seed for deterministic results #########\n",
    "\n",
    "SEED = 89\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## policy network/agent\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, in_sz, nh, out_sz):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_sz, nh) # weights check the positon of paddles and ball \n",
    "        self.fc_out = nn.Linear(nh, out_sz) # decide to go up or down\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [batch_sz x in_sz] images after (pre-processing)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        lop_p = self.fc_out(h)\n",
    "        return lop_p\n",
    "\n",
    "# functions\n",
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data)\n",
    "\n",
    "def save_checkpoint(m, ts, m_pth, ver):\n",
    "    torch.save(m.state_dict(), os.path.join(m_pth, f\"{m.__class__.__name__}_{ts}_{ver}.pth\"))\n",
    "    print(f\"Model saved to {m_pth} as {m.__class__.__name__}_{ts}_{ver}.pth\")\n",
    "    \n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    \n",
    "    # crop image\n",
    "    I = I[35:195]\n",
    "    \n",
    "    # downsample, take every second element and set to grey scale \n",
    "    I = I[::2,::2,0]\n",
    "    \n",
    "    I[I == 144] = 0 # erase background (type I) set to black \n",
    "    I[I == 109] = 0 # erase background (type II) set to black \n",
    "    \n",
    "    # everything else erase (ball, paddle)\n",
    "    I[I != 0] = 1 \n",
    "    \n",
    "    return I.astype(np.float32).ravel()\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Take input 1D float array of rewards and return discounted rewards.\"\"\"\n",
    "\n",
    "    # placeholder for discounted rewards\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    \n",
    "    running_sum = 0\n",
    "    for t in range(len(r)-1, 0, -1):\n",
    "        if r[t] != 0.0: running_sum = 0\n",
    "        running_sum = running_sum * gamma + r[t]\n",
    "        discounted_r[t] = running_sum\n",
    "        \n",
    "    discounted_r -= np.mean(discounted_r)\n",
    "    discounted_r /= np.std(discounted_r)\n",
    "    \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read hyperparams file\n",
    "with open(\"config-v1.json\") as json_f:\n",
    "    config = json.load(json_f)\n",
    "\n",
    "# instantiate tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# instantiate env obj\n",
    "env = gym.make(config[\"env_name\"])\n",
    "render_env = config[\"render\"]\n",
    "\n",
    "gamma = config[\"gamma\"]\n",
    "save_every = config[\"save_every\"]\n",
    "rollout_sz = config[\"rollout_sz\"]\n",
    "n_epochs = config[\"n_epochs\"]\n",
    "ent_beta = config[\"ent_beta\"]\n",
    "\n",
    "# device to use\n",
    "# dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(f\"Runnign on device {dev}\")\n",
    "\n",
    "# agent to drive our paddle \n",
    "policy_net = Agent(config[\"agent\"][\"in_sz\"], config[\"agent\"][\"nh\"],env.action_space.n) \n",
    "policy_net.apply(init_weights)\n",
    "policy_net.to(device)\n",
    "\n",
    "# Adam for agent weight updates\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(batch_logits, batch_weighted_log_p, beta=0.1):\n",
    "    policy_loss = -1 * torch.mean(batch_weighted_log_p)\n",
    "    \n",
    "    # add the entropy bonus\n",
    "    p = softmax(batch_logits, dim=1)\n",
    "    \n",
    "    log_p = log_softmax(batch_logits, dim=1)\n",
    "    \n",
    "    entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "    \n",
    "    entropy_bonus = -1 * beta * entropy\n",
    "    \n",
    "    return policy_loss + entropy_bonus\n",
    "\n",
    "\n",
    "## run experiments \n",
    "def train():\n",
    "    epoch = 0\n",
    "    while epoch <= n_epochs:\n",
    "        ep_n = 0\n",
    "        \n",
    "        w_running_reward = None\n",
    "        # buffers\n",
    "        batch_weighted_log_p = torch.empty(size=(0,), dtype=torch.float, device=device)\n",
    "        batch_logits = torch.empty(size=(0, env.action_space.n), device=device)\n",
    "\n",
    "        while ep_n < save_every:\n",
    "            # play episode and collect loss and logits\n",
    "            episode_weighted_log_p, episode_logits, episode_rewards = play_ep()\n",
    "            w_running_reward = np.sum(episode_rewards) if not w_running_reward else w_running_reward * 0.99 + 0.01 * np.sum(episode_rewards)\n",
    "            batch_weighted_log_p = torch.cat((batch_weighted_log_p, episode_weighted_log_p), dim=0)\n",
    "            batch_logits = torch.cat((batch_logits, episode_logits), dim=0)\n",
    "            ep_n += 1\n",
    "            if ep_n % 10 == 0:\n",
    "                print(f\"Ep_no: {ep_n}\")\n",
    "\n",
    "            if ep_n % rollout_sz == 0:\n",
    "                # compute loss \n",
    "                loss = calculate_loss(batch_logits, batch_weighted_log_p, ent_beta)\n",
    "                # zero the grads\n",
    "                optimizer.zero_grad()\n",
    "                # backprop\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(f\"batch loss {loss}\")\n",
    "                # reset the epoch arrays\n",
    "                # used for entropy calculation\n",
    "                batch_logits = torch.empty(size=(0, env.action_space.n), device=device)\n",
    "                batch_weighted_log_p = torch.empty(size=(0,), dtype=torch.float, device=device)\n",
    "                print(f\"Running weighted reward {w_running_reward}\")\n",
    "\n",
    "\n",
    "        cdir = os.getcwd()\n",
    "        ts = datetime.now().strftime(\"%m.%d.%Y.%H_%M_%S\")\n",
    "        save_checkpoint(policy_net, ts, os.path.join(cdir, \"saved_weights\"), epoch)  \n",
    "        epoch += 1\n",
    "        print(f\"epoch no {epoch}\")\n",
    "\n",
    "## play episode\n",
    "def play_ep():\n",
    "    # reset env state after every episode\n",
    "    state = env.reset() \n",
    "    prev_x = None\n",
    "    episode_actions = torch.empty(size=(0,), dtype=torch.long,device=device)\n",
    "    episode_logits = torch.empty(size=(0, env.action_space.n),device=device)\n",
    "    average_rewards = np.empty(shape=(0,), dtype=np.float)\n",
    "    episode_rewards = np.empty(shape=(0,), dtype=np.float)\n",
    "    \n",
    "    while True:\n",
    "        # render env for display \n",
    "        if render_env:\n",
    "            env.render()\n",
    "\n",
    "        # pre-preprocess current the state and subtract fromprevious state to add in motion information\n",
    "        cur_x = prepro(state)    \n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(config[\"agent\"][\"in_sz\"]).astype(np.float32)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        # get choice from network\n",
    "        action_log_p = policy_net(torch.tensor(x).float().unsqueeze(0).to(device))\n",
    "        # add to buffer\n",
    "        episode_logits = torch.cat((episode_logits, action_log_p),dim=0)\n",
    "        # sample and action and execute the action \n",
    "        action = Categorical(logits=action_log_p).sample()\n",
    "        # add to buffer\n",
    "        episode_actions = torch.cat((episode_actions, action),dim=0)\n",
    "\n",
    "        state, reward, done, _ = env.step(action=action.cpu().item())\n",
    "\n",
    "        # add to buffer \n",
    "        episode_rewards = np.concatenate((episode_rewards, np.array([reward])), axis=0)\n",
    "\n",
    "        # average rewards after every action and add to buffer for computing baseline\n",
    "        # like averaging from 1 to nth time step (on-average return till that time step)\n",
    "        average_rewards = np.concatenate((average_rewards, np.expand_dims(np.mean(episode_rewards), axis=0)), axis=0)\n",
    "            \n",
    "        if done: # end of episode\n",
    "            # get discounted rewards and normalize the return\n",
    "            discounted_rewards = discount_rewards(episode_rewards, gamma=gamma)\n",
    "                \n",
    "            # subtract baseline rewards \n",
    "            discounted_rewards -= average_rewards\n",
    "                \n",
    "            # set mask for the actions executed \n",
    "            mask = one_hot(episode_actions, num_classes=env.action_space.n)\n",
    "                \n",
    "            # similar to cross-entropy for classification but with fake labels and our action confidence\n",
    "            episode_loss = torch.sum(mask.float() * log_softmax(episode_logits, dim=1), dim=1)\n",
    "                \n",
    "            # weight the loss with the discounted rewards to get expected reward from distribution \n",
    "            episode_weighted_log_p = episode_loss * torch.tensor(discounted_rewards).float().to(device)\n",
    "            \n",
    "            return episode_weighted_log_p, episode_logits, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda7936eaf21be644f28e34e9ca0765a6d1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
