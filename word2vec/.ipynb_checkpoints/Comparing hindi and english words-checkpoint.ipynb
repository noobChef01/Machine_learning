{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import os \n",
    "from multiprocessing import Pool\n",
    "from glob import glob\n",
    "from googletrans import Translator\n",
    "import itertools\n",
    "from fastai.text import *\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/data/'\n",
    "files = glob(path+'*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_punc(text):\n",
    "    pattern = r'[.,!]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_corpus(docs):\n",
    "    tokens = [doc.split() for doc in docs]\n",
    "    return tokens\n",
    "\n",
    "def get_vocab(tok_texts):\n",
    "    all_tokens = list(itertools.chain(*tok_texts))\n",
    "    vocabulary = sorted(list(set(all_tokens)))\n",
    "    vocabulary.insert(0, '/0')\n",
    "    word2idx = {w: idx for idx, w in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for idx, w in enumerate(vocabulary)}\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    return vocabulary, vocabulary_size, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "hin_texts = [open(file, 'r', encoding=\"utf-8\").read() for file in files] \n",
    "eng_texts = [translator.translate(text,dest='en').text for text in hin_texts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(3)\n",
    "# remove punctuations\n",
    "hin_texts = pool.map(rm_punc, hin_texts)\n",
    "eng_texts = pool.map(rm_punc, eng_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized list\n",
    "hin_tok = tokenize_corpus(hin_texts)\n",
    "eng_tok = tokenize_corpus(eng_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hindi parameters:\n",
    "hvocab, h_vs, hw2i, hi2w  = get_vocab(hin_tok)\n",
    "# english paramaters:\n",
    "evocab, e_vs, ew2i, ei2w  = get_vocab(eng_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_skipg(wds, tokens, word2idx):\n",
    "    xs, ys = [], []\n",
    "    for i, tok in enumerate(tokens):\n",
    "        t_c = []\n",
    "        t_tr = word2idx[tokens[i]]\n",
    "        for j in range(1,wds+1):      \n",
    "            if i+j < len(tokens):\n",
    "                t_c.append(word2idx[tokens[i+j]])\n",
    "            if i-j > -1:\n",
    "                t_c.append(word2idx[tokens[i-j]])           \n",
    "        ys.append(t_c)\n",
    "        xs.append([t_tr]*len(t_c))\n",
    "    xs = list(itertools.chain(*xs))\n",
    "    xs.insert(0, xs[0])\n",
    "    xs.insert(-1, xs[-1])\n",
    "    ys = list(itertools.chain(*ys))\n",
    "    ys.insert(0, 0)\n",
    "    ys.append(0)\n",
    "    return xs, ys\n",
    "\n",
    "def gen_data(wds, tokenized_corpus, word2idx):\n",
    "    xs = []; ys = []\n",
    "    for i in range(len(tokenized_corpus)):\n",
    "        xt, yc = gen_skipg(wds, tokenized_corpus[i], word2idx)\n",
    "        xs.append(xt); ys.append(yc)\n",
    "    xs = list(itertools.chain(*xs))\n",
    "    ys = list(itertools.chain(*ys))\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hindi skipgram data\n",
    "hxs, hys = gen_data(4, hin_tok, hw2i)\n",
    "hxs = np.array(hxs).reshape(-1,1)\n",
    "hys = np.array(hys)\n",
    "\n",
    "# eng skipgram data\n",
    "exs, eys = gen_data(4, eng_tok, ew2i)\n",
    "exs = np.array(exs).reshape(-1,1)\n",
    "eys = np.array(eys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
