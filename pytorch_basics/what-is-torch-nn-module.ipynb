{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04cf257d0d697206bc3fd9ea9972d00920bfe789"
   },
   "source": [
    "### Load the data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = Path(\"../input/mnist-in-csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "40326ce8186784102e023a7da150d27c5c185d25"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_PATH / 'mnist_train.csv') \n",
    "test = pd.read_csv(DATA_PATH / 'mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "766f25e00babb0a18bdbb6bd75c6fb19c8a29ffc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "802a2e22b44f4551815d91a12f3ec390ffa14881"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37d7b713d3a9f95bbb7283e3c3a6181977080e14"
   },
   "source": [
    "### Organize the data into a suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "bd28e4852422f8247f6af80f7e6a53903dc7e72b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train, test_size=0.1, stratify=train['label'])\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "valid.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "62877288d5d41b5bdba41d598fecf6f816c07048"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = train.iloc[:, 1:].values , train.iloc[:, 0].values\n",
    "x_val, y_val = valid.iloc[:, 1:].values , valid.iloc[:, 0].values\n",
    "x_test = test.iloc[:, :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "59d793aef9f45a53ef8aff206450bc4009af535e"
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4f335ffd2795a8d2fdf6d91df8763648608c34e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test, valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e01f0a32fb78708f0141e074b32e0c3c0a548d7b"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5d03befccac5727b8e40d8bc9d5eb17446bac2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chech the data\n",
    "plt.imshow(x_train[0].reshape((28,28)), cmap='gray')\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "dd4fd81949f9203311690d7a6c5c194935f6c1cd"
   },
   "outputs": [],
   "source": [
    "# convert the data to tensors inorder to use pytorch\n",
    "import torch\n",
    "x_train,x_val, x_test = map(torch.FloatTensor, (x_train,x_val, x_test))\n",
    "n, c = x_train.shape\n",
    "y_train, y_val = map(torch.IntTensor, (y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e3fd6a211a9e2634539382bd62032f5daa752d96"
   },
   "outputs": [],
   "source": [
    "# initialize the weights and bias\n",
    "import math\n",
    "weights = torch.randn((784,10)) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8ad4917b18aacdbf622a4246940e1db8e94b9a6"
   },
   "source": [
    "### Create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "2f99f5daea42b79db00bb181cd54113c6c9b2cea"
   },
   "outputs": [],
   "source": [
    "### in pytorch any function or object can act asa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8106e4801b03cc184d57897318c4fd095532f0e3"
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "0e6142baaceee7265a1927cbf216ea399696d6f3"
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ca56e0a3dfa6a355d0babb860c3c1307fd79a824"
   },
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.type(torch.long).shape[0]), target.type(torch.long)].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "69255be55e5f86a4cfd437540268e008edc2f944"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the loss\n",
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a6fc513ee8b77bb929057c5b96110759501fac0d"
   },
   "outputs": [],
   "source": [
    "# function for accuracy\n",
    "def accuracy(out,yb):\n",
    "    preds = torch.argmax(out,dim=1)\n",
    "    return (preds == yb.type(torch.long)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "ee1bac1bf199cf579c5e88707893ee8589675e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1406)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b1a0d1cdd72ed5ed811c32297a1e7b935ba2f6c0"
   },
   "outputs": [],
   "source": [
    "# let's write our training loop\n",
    "lr_rate = 0.5\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        starti = i * bs\n",
    "        endi = starti + bs\n",
    "        xb = x_train[starti: endi]\n",
    "        yb = y_train[starti: endi]\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        \n",
    "        loss.backward()        \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr_rate\n",
    "            bias -= bias.grad * lr_rate\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "e994f1f06b2c453c1513804590f474316f583ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<NegBackward>) tensor(0.0417)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "093e210fd8727de5e40fad973bc4bf5a86f2d92f"
   },
   "outputs": [],
   "source": [
    "## let's use the torch.nn.functional module which conatains many torch functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "42b5c8f4f69a90dbfb265997b2724354b2696961"
   },
   "outputs": [],
   "source": [
    "# cross-entropy loss is a combination of logsoftmax and nll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "d93bf79c39e700ad71695d687212ffc39e9deccc"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "bb2d67fa43309b0cf0fd30c1c7f8ba7448f43275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, grad_fn=<NllLossBackward>) tensor(0.0417)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb.type(torch.long)), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "e5af2f2a4275e475fdc489cae3b2158a0110a56b"
   },
   "outputs": [],
   "source": [
    "## let's use the nn.Module and nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "5d8842267d562fc5ab233e2657c26615a626db5b"
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "c9176343f31a494365bda0838b1f503b46ad9d82"
   },
   "outputs": [],
   "source": [
    "class MNIST_logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn((784,10)) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10)) # requires grad is taken care of\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "d39697ead30d3974a1359ee1cf3ec3a524f9fdcb"
   },
   "outputs": [],
   "source": [
    "model = MNIST_logistic() # the class can be used as a function and forward is automaticaly called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "c4d3d7f718c27970453bb1cdf946920ddc664e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(119.1691, grad_fn=<NllLossBackward>) tensor(0.0833)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "1de90ec952d1b7e75cb9b41ae649497413e85720"
   },
   "outputs": [],
   "source": [
    "def fit(lr_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            starti = i * bs\n",
    "            endi = starti + bs\n",
    "            xb = x_train[starti: endi]\n",
    "            yb = y_train[starti: endi]\n",
    "            loss = loss_func(model(xb), yb.type(torch.long))\n",
    "\n",
    "            loss.backward()        \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr_rate\n",
    "                model.zero_grad() # all parmaters grad is made 0\n",
    "                \n",
    "fit(0.05, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "2c1862c6eb93a391b9983d8c60a98dc911a55bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(72.5121, grad_fn=<NllLossBackward>) tensor(0.9583)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "fe459c6384f7a37179501ed4182a423d922b8863"
   },
   "outputs": [],
   "source": [
    "# refactor our model\n",
    "class MNIST_logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10) # take care of initialization and requires grad \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "0c85f7f94cf0059fe6804719147b749a9e890611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.6374, grad_fn=<NllLossBackward>) tensor(0.0417)\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_logistic()\n",
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "43b11da7aa7261d21999eb06cca7fbd1137e01d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(319.9439, grad_fn=<NllLossBackward>) tensor(0.9375)\n"
     ]
    }
   ],
   "source": [
    "fit(0.05, 2)\n",
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96bff6e18bed178ef0f0e24f811745dc2ef18fbd"
   },
   "source": [
    "# refactor using torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "e78b6ab221e28763954a223f7a6030a080ee30fb"
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "6c5cd0f13e13c5d5b01fe5af375b52cdee91ce49"
   },
   "outputs": [],
   "source": [
    "def get_model(lr):\n",
    "    model = MNIST_logistic()\n",
    "    return model, optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "model,opt = get_model(0.05)\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            starti = i * bs\n",
    "            endi = starti + bs\n",
    "            xb = x_train[starti: endi]\n",
    "            yb = y_train[starti: endi]\n",
    "            loss = loss_func(model(xb), yb.type(torch.long))\n",
    "            loss.backward()       \n",
    "            opt.step() #for all parms take forward step\n",
    "            opt.zero_grad() # zerod the grads for next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "f3663f707321afdf19bb818feaf3b9ff307fdc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59.1030, grad_fn=<NllLossBackward>) tensor(0.1667)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "e13e89ee18858d47561bf69dc007e3b8019244c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.0636, grad_fn=<NllLossBackward>) tensor(0.9583)\n"
     ]
    }
   ],
   "source": [
    "fit(2)\n",
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "0a13d16cf405358f0ba025ff97f60751eaf483f2"
   },
   "outputs": [],
   "source": [
    "# refactor using dataset\n",
    "from torch.utils.data import TensorDataset # dataset wrapper aroung tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "48b890a9a6799f661a62cacf50dbae73db7f0833"
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "c0460ee989e57f4ccd7c58433f3aec14b53d41d6"
   },
   "outputs": [],
   "source": [
    "model,opt = get_model(0.05)\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            xb, yb = train_ds[i*bs: i*bs+bs]\n",
    "            loss = loss_func(model(xb), yb.type(torch.long))\n",
    "            loss.backward()       \n",
    "            opt.step() #for all parms take forward step\n",
    "            opt.zero_grad() # zerod the grads for next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "f3b4958d2cce76a001cb9b218f8da681b324328d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(84.8455, grad_fn=<NllLossBackward>) tensor(0.0833)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "91b8553a0e70b81bbc0779f501e2b0e140c81fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50.7047, grad_fn=<NllLossBackward>) tensor(0.9583)\n"
     ]
    }
   ],
   "source": [
    "fit(2)\n",
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "3f338b7d04fc792039e470a9d2c1025f55409a03"
   },
   "outputs": [],
   "source": [
    "# refactor using dataloader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "ee7d3d82e4c7b31e6f6210721ddc16813675f16d"
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "eea3a2a179c0580febe3fa8d0f7f14a399271e14"
   },
   "outputs": [],
   "source": [
    "model,opt = get_model(0.05)\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb.type(torch.long))\n",
    "            loss.backward()       \n",
    "            opt.step() #for all parms take forward step\n",
    "            opt.zero_grad() # zerod the grads for next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "8622f0263300fcfc63d0e66a5f7ccb4be75d767e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.6245, grad_fn=<NllLossBackward>) tensor(0.1042)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "e0181f1dbf2826e4850fc142bb58c424f05e6639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(47.4212, grad_fn=<NllLossBackward>) tensor(0.9167)\n"
     ]
    }
   ],
   "source": [
    "fit(2)\n",
    "print(loss_func(model(xb),yb.type(torch.long)), accuracy(model(xb),yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "f9b675519f288759fbf4ddf35e8fd8a5e7e8a9fe"
   },
   "outputs": [],
   "source": [
    "# let's put up a validatin set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "a20bdbf2795c766c311ee8fa4f9d5c82c30489e4"
   },
   "outputs": [],
   "source": [
    "# important to shuffle training data to prevent correlation in minibaches but validation shuffling is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "9c4912c63ba0331f7f1d76a3080f6353173fe65f"
   },
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_ds = TensorDataset(x_val, y_val)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=2*bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "991c3c2ffcc4980247796a4cbd528562b1f1e345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 281.6712951660156\n",
      "1 223.49269104003906\n",
      "2 199.86712646484375\n",
      "3 232.7772979736328\n"
     ]
    }
   ],
   "source": [
    "model,opt = get_model(0.05)\n",
    "\n",
    "def fit(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # imp to put for traning and eval time for batchnorm and dropout\n",
    "        for xb, yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb.type(torch.long))\n",
    "            loss.backward()       \n",
    "            opt.step() #for all parms take forward step\n",
    "            opt.zero_grad() # zerod the grads for next batch\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_func(model(xb), yb.type(torch.long)) for xb, yb in valid_dl)\n",
    "        print(epoch, (valid_loss / len(valid_dl)).item()) # number of minibatches\n",
    "fit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "43df9a20d3f684be0b5777be907734b1b54005ba"
   },
   "outputs": [],
   "source": [
    "# more refactoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "169ba1e8e6bd43c197b8980587eaf7e111404523"
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=2*bs))\n",
    "\n",
    "def loss_batch(model, loss_func,xb, yb,opt=None):\n",
    "    loss = loss_func(model(xb), yb.type(torch.long))\n",
    "    \n",
    "    if opt:\n",
    "        loss.backward()\n",
    "        opt.step() #for all parms take forward step\n",
    "        opt.zero_grad() # zerod the grads for next batch\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "f2b54722a8aea13d07c04ce9941033d657d3257b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # imp to put for traning and eval time for batchnorm and dropout\n",
    "        tr_losses, tr_bsz = [], []\n",
    "        for xb, yb in train_dl:\n",
    "            loss, bs = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            tr_losses.append(loss); tr_bsz.append(bs)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, bsz = zip(*(loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl))\n",
    "        valid_loss = np.sum(np.multiply(losses, bsz)) / np.sum(bsz)\n",
    "        train_loss = np.sum(np.multiply(tr_losses, tr_bsz)) / np.sum(tr_bsz)\n",
    "        print(f\"epoch:{epoch}\", f'train_loss:{train_loss}',f'valid_loss:{valid_loss}')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "d0253b7f26c921a886a78cccbe5357a3fd70117e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train_loss:267.0352377771448 valid_loss:245.3351257324219\n",
      "epoch:1 train_loss:214.4493124762641 valid_loss:228.47414021809897\n",
      "epoch:2 train_loss:214.78188225244594 valid_loss:249.67978043619792\n",
      "epoch:3 train_loss:213.51757204635055 valid_loss:279.5303752441406\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model(0.05)\n",
    "fit(4, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "82e0d2a67033fbd3d58070c8e80822e71f0e2fea"
   },
   "outputs": [],
   "source": [
    "# let's switch to a CNN\n",
    "\n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "ed802f377e9483f1709288998c1bfca4e86d7541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train_loss:1.8894874581231011 valid_loss:1.5759652614593507\n",
      "epoch:1 train_loss:1.6056821239259509 valid_loss:1.7554696782430013\n",
      "epoch:2 train_loss:1.605747270937319 valid_loss:1.6840614191691081\n",
      "epoch:3 train_loss:1.6150890464782715 valid_loss:1.675480343500773\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_CNN()\n",
    "opt = optim.Adam(model.parameters(), lr=0.05, betas=(0.8, 0.7))\n",
    "\n",
    "fit(4, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor using nn.Sequential\n",
    "# define custm layer using function\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def reshape(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "a9e241ff29cbf248b6c9fee5b74a656bac1ad7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train_loss:2.3959516685627125 valid_loss:2.3025827452341714\n",
      "epoch:1 train_loss:2.302583456463284 valid_loss:2.3025827452341714\n",
      "epoch:2 train_loss:2.302583456463284 valid_loss:2.3025827452341714\n",
      "epoch:3 train_loss:2.302583456463284 valid_loss:2.3025827452341714\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "                    Lambda(reshape),\n",
    "                    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AvgPool2d(4),\n",
    "                    Lambda(lambda x: x.view(x.size(0), -1))   \n",
    "                     )\n",
    "opt = optim.Adam(model.parameters(), lr=0.05, betas=(0.8, 0.7))\n",
    "\n",
    "fit(4, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping the dataloader\n",
    "class WrapperDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield self.func(*b)\n",
    "            \n",
    "def reshape(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrapperDataLoader(train_dl, reshape)\n",
    "valid_dl = WrapperDataLoader(valid_dl, reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train_loss:2.3236872588970043 valid_loss:2.3025827452341714\n",
      "epoch:1 train_loss:2.302583456463284 valid_loss:2.3025827452341714\n",
      "epoch:2 train_loss:2.302583456463284 valid_loss:2.3025827452341714\n",
      "epoch:3 train_loss:2.302583456463284 valid_loss:2.3025827452341714\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "                    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool2d(1),\n",
    "                    Lambda(lambda x: x.view(x.size(0), -1))   \n",
    "                     )\n",
    "opt = optim.Adam(model.parameters(), lr=0.05, betas=(0.8, 0.7))\n",
    "\n",
    "fit(4, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up device\n",
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor code to use GPU\n",
    "def reshape(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrapperDataLoader(train_dl, reshape)\n",
    "valid_dl = WrapperDataLoader(valid_dl, reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train_loss:2.445718612247043 valid_loss:2.3025849043528237\n",
      "epoch:1 train_loss:2.3025848859151203 valid_loss:2.3025849043528237\n",
      "epoch:2 train_loss:2.3025848859151203 valid_loss:2.3025849043528237\n",
      "epoch:3 train_loss:2.3025848859151203 valid_loss:2.3025849043528237\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "                    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool2d(1),\n",
    "                    Lambda(lambda x: x.view(x.size(0), -1))   \n",
    "                     )\n",
    "model.to(dev)\n",
    "opt = optim.Adam(model.parameters(), lr=0.05, betas=(0.8, 0.7))\n",
    "fit(4, model, loss_func, opt, train_dl, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
